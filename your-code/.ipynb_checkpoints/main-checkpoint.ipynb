{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Import-and-Describe-the-Dataset\" data-toc-modified-id=\"Challenge-1---Import-and-Describe-the-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Import and Describe the Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?\" data-toc-modified-id=\"Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the dataset with mathematical and visualization techniques. What do you find?</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Data-Cleaning-and-Transformation\" data-toc-modified-id=\"Challenge-2---Data-Cleaning-and-Transformation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Data Cleaning and Transformation</a></span></li><li><span><a href=\"#Challenge-3---Data-Preprocessing\" data-toc-modified-id=\"Challenge-3---Data-Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Data Preprocessing</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.\" data-toc-modified-id=\"We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>We will use the <code>StandardScaler</code> from <code>sklearn.preprocessing</code> and scale our data. Read more about <code>StandardScaler</code> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" target=\"_blank\">here</a>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Data-Clustering-with-K-Means\" data-toc-modified-id=\"Challenge-4---Data-Clustering-with-K-Means-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Data Clustering with K-Means</a></span></li><li><span><a href=\"#Challenge-5---Data-Clustering-with-DBSCAN\" data-toc-modified-id=\"Challenge-5---Data-Clustering-with-DBSCAN-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Data Clustering with DBSCAN</a></span></li><li><span><a href=\"#Challenge-6---Compare-K-Means-with-DBSCAN\" data-toc-modified-id=\"Challenge-6---Compare-K-Means-with-DBSCAN-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Compare K-Means with DBSCAN</a></span></li><li><span><a href=\"#Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters\" data-toc-modified-id=\"Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge 2 - Changing K-Means Number of Clusters</a></span></li><li><span><a href=\"#Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples\" data-toc-modified-id=\"Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Bonus Challenge 3 - Changing DBSCAN <code>eps</code> and <code>min_samples</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings                                              \n",
    "from sklearn.exceptions import DataConversionWarning          \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Import and Describe the Dataset\n",
    "\n",
    "In this lab, we will use a dataset containing information about customer preferences. We will look at how much each customer spends in a year on each subcategory in the grocery store and try to find similarities using clustering.\n",
    "\n",
    "The origin of the dataset is [here](https://archive.ics.uci.edu/ml/datasets/wholesale+customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset into a DataFrame with the corrected path\n",
    "file_path = r\"C:\\Users\\jesus\\lab-unsupervised-learning-en\\data\\Wholesale customers data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Get basic information about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "print(data.info())\n",
    "\n",
    "# Get a statistical summary of numerical columns\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Visualize correlations between numerical features\n",
    "print(\"\\nCorrelation Heatmap:\")\n",
    "sns.heatmap(data.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Dataset summary\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(f\"Number of rows: {data.shape[0]}\")\n",
    "print(f\"Number of columns: {data.shape[1]}\")\n",
    "\n",
    "print(\"\\nColumn names and data types:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "print(\"\\nInsights:\")\n",
    "print(\"1. The dataset has numerical features representing annual spending in various categories.\\n\"\n",
    "      \"2. High correlations might indicate redundancy between features.\\n\"\n",
    "      \"3. Check for outliers and normalize data for clustering in the next steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset with mathematical and visualization techniques. What do you find?\n",
    "\n",
    "Checklist:\n",
    "\n",
    "* What does each column mean?\n",
    "* Any categorical data to convert?\n",
    "* Any missing data to remove?\n",
    "* Column collinearity - any high correlations?\n",
    "* Descriptive statistics - any outliers to remove?\n",
    "* Column-wise data distribution - is the distribution skewed?\n",
    "* Etc.\n",
    "\n",
    "Additional info: Over a century ago, an Italian economist named Vilfredo Pareto discovered that roughly 20% of the customers account for 80% of the typical retail sales. This is called the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle). Check if this dataset displays this characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "file_path = r\"C:\\Users\\jesus\\lab-unsupervised-learning-en\\data\\Wholesale customers data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Get basic information about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "print(data.info())\n",
    "\n",
    "# Get a statistical summary of numerical columns\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Visualize correlations between numerical features\n",
    "print(\"\\nCorrelation Heatmap:\")\n",
    "sns.heatmap(data.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Visualizing distribution of each column\n",
    "print(\"\\nColumn-wise Data Distribution (Histograms):\")\n",
    "data.hist(figsize=(12, 10), bins=20, color='skyblue', edgecolor='black')\n",
    "plt.suptitle(\"Distribution of Data Columns\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Boxplot to identify outliers\n",
    "print(\"\\nBoxplot to Identify Outliers:\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=data, width=0.5, palette=\"Set2\")\n",
    "plt.title(\"Boxplot of Data Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Pareto analysis - Checking if the 80/20 rule holds\n",
    "total_spent = data.sum(axis=1)  # Sum of all spending per customer\n",
    "sorted_spent = total_spent.sort_values(ascending=False)\n",
    "cumulative_percentage = sorted_spent.cumsum() / sorted_spent.sum() * 100\n",
    "\n",
    "# Plot Pareto (80/20 rule)\n",
    "print(\"\\nPareto Analysis - Cumulative Spending Percentage:\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sorted_spent.index, cumulative_percentage, color='orange', marker='o', linestyle='dashed', linewidth=2)\n",
    "plt.axhline(y=80, color='red', linestyle='--')\n",
    "plt.title(\"Pareto Analysis: Cumulative Spending Percentage\")\n",
    "plt.xlabel(\"Customer Index\")\n",
    "plt.ylabel(\"Cumulative Percentage of Total Spending\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "- ex.: Frozen, Grocery, Milk and Detergents Paper have a high...\n",
    "- ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Data Cleaning and Transformation\n",
    "\n",
    "If your conclusion from the previous challenge is the data need cleaning/transformation, do it in the cells below. However, if your conclusion is the data need not be cleaned or transformed, feel free to skip this challenge. But if you do choose the latter, please provide rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "file_path = r\"C:\\Users\\jesus\\lab-unsupervised-learning-en\\data\\Wholesale customers data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 1. Handle Missing Data\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# If there are missing values, we can fill them with the median (you can choose mean, mode, or remove rows depending on context)\n",
    "# In this case, we'll fill missing values with the median of each column:\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# 2. Handling Outliers - Using IQR method to remove extreme outliers\n",
    "# Calculate the IQR for each column\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define a threshold for detecting outliers (1.5 * IQR rule)\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Remove outliers\n",
    "data_no_outliers = data[~((data < lower_bound) | (data > upper_bound)).any(axis=1)]\n",
    "\n",
    "print(\"\\nShape before and after removing outliers:\")\n",
    "print(f\"Original shape: {data.shape}\")\n",
    "print(f\"Shape after outlier removal: {data_no_outliers.shape}\")\n",
    "\n",
    "# 3. Data Normalization/Standardization (if needed for clustering)\n",
    "# Apply StandardScaler to normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_no_outliers)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame for better readability\n",
    "data_scaled_df = pd.DataFrame(data_scaled, columns=data.columns)\n",
    "\n",
    "# Check the scaled data\n",
    "print(\"\\nSample of scaled data:\")\n",
    "print(data_scaled_df.head())\n",
    "\n",
    "# 4. Skewed Data Transformation (e.g., log transformation)\n",
    "# Identify any skewed data using skewness\n",
    "skewed_columns = data_no_outliers.skew()\n",
    "print(\"\\nSkewness of the columns:\")\n",
    "print(skewed_columns)\n",
    "\n",
    "# Apply log transformation for highly skewed columns (with skewness > 1 or < -1)\n",
    "for column in skewed_columns[skewed_columns > 1].index:\n",
    "    data_no_outliers.loc[:, column] = np.log1p(data_no_outliers[column])\n",
    "\n",
    "# Check the skewness again after transformation\n",
    "skewed_columns_after = data_no_outliers.skew()\n",
    "print(\"\\nSkewness of the columns after transformation:\")\n",
    "print(skewed_columns_after)\n",
    "\n",
    "# Summary of transformations\n",
    "print(\"\\nData cleaning and transformation summary:\")\n",
    "print(\"1. Missing values filled with the median.\")\n",
    "print(\"2. Outliers removed using the IQR method.\")\n",
    "print(\"3. Data normalized using StandardScaler for clustering purposes.\")\n",
    "print(\"4. Skewed data transformed using log transformation.\")\n",
    "\n",
    "# Visualizing boxplot after data cleaning\n",
    "print(\"\\nBoxplot to Identify Outliers (Post-cleaning):\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=data_no_outliers, width=0.5, palette=\"Set2\")\n",
    "plt.title(\"Boxplot of Data Features After Data Cleaning\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "-  ...\n",
    "-  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Data Preprocessing\n",
    "\n",
    "One problem with the dataset is the value ranges are remarkably different across various categories (e.g. `Fresh` and `Grocery` compared to `Detergents_Paper` and `Delicassen`). If you made this observation in the first challenge, you've done a great job! This means you not only completed the bonus questions in the previous Supervised Learning lab but also researched deep into [*feature scaling*](https://en.wikipedia.org/wiki/Feature_scaling). Keep on the good work!\n",
    "\n",
    "Diverse value ranges in different features could cause issues in our clustering. The way to reduce the problem is through feature scaling. We'll use this technique again with this dataset.\n",
    "\n",
    "#### We will use the `StandardScaler` from `sklearn.preprocessing` and scale our data. Read more about `StandardScaler` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "*After scaling your data, assign the transformed data to a new variable `customers_scale`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling to the cleaned data (data_no_outliers) and assign it to customers_scale\n",
    "customers_scale = scaler.fit_transform(data_no_outliers)\n",
    "\n",
    "# Convert the scaled data into a DataFrame for easier inspection\n",
    "customers_scale_df = pd.DataFrame(customers_scale, columns=data_no_outliers.columns)\n",
    "\n",
    "# Display the first few rows of the scaled data\n",
    "print(\"\\nSample of scaled data:\")\n",
    "print(customers_scale_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Data Clustering with K-Means\n",
    "\n",
    "Now let's cluster the data with K-Means first. Initiate the K-Means model, then fit your scaled data. In the data returned from the `.fit` method, there is an attribute called `labels_` which is the cluster number assigned to each data record. What you can do is to assign these labels back to `customers` in a new column called `customers['labels']`. Then you'll see the cluster results of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import KMeans from sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set the number of clusters. You can experiment with this value.\n",
    "# Here, we'll start with 4 clusters as an example.\n",
    "n_clusters = 4\n",
    "\n",
    "# Initialize the KMeans model\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Fit the KMeans model to the scaled data\n",
    "kmeans.fit(customers_scale)\n",
    "\n",
    "# Assign the cluster labels to the original data in a new column 'labels'\n",
    "customers_scale_df['labels'] = kmeans.labels_\n",
    "\n",
    "# Display the first few rows of the data with the cluster labels\n",
    "print(\"\\nSample of clustered data:\")\n",
    "print(customers_scale_df.head())\n",
    "\n",
    "# Optionally, you can visualize the clusters (2D visualization for illustration)\n",
    "# Plotting the first two principal components or using pairplot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the labels to plot the clusters (for 2D visualization)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=customers_scale_df.iloc[:, 0], y=customers_scale_df.iloc[:, 1], hue=customers_scale_df['labels'], palette='Set2')\n",
    "plt.title(\"K-Means Clustering (2D Visualization of First Two Features)\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking to the elbow we can choose 2 like the correct number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(n_clusters=2).fit(customers_scale)\n",
    "\n",
    "labels = kmeans_2.predict(customers_scale)\n",
    "\n",
    "clusters = kmeans_2.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_customers['Label'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans from sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit the K-Means model with 2 clusters\n",
    "kmeans_2 = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans_2.fit(customers_scale)\n",
    "\n",
    "# Assign the cluster labels to the data\n",
    "labels = kmeans_2.predict(customers_scale)  # or use kmeans_2.labels_\n",
    "clusters = kmeans_2.labels_.tolist()\n",
    "\n",
    "# Add the labels column to the cleaned data using .loc to avoid SettingWithCopyWarning\n",
    "data_no_outliers.loc[:, 'Label'] = clusters\n",
    "\n",
    "# Display the first few rows to check the labels\n",
    "print(\"\\nSample of the customers with cluster labels:\")\n",
    "print(data_no_outliers.head())\n",
    "\n",
    "# Count the number of data points in each cluster\n",
    "cluster_counts = data_no_outliers['Label'].value_counts()\n",
    "print(\"\\nCluster distribution:\")\n",
    "print(cluster_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Data Clustering with DBSCAN\n",
    "\n",
    "Now let's cluster the data using DBSCAN. Use `DBSCAN(eps=0.5)` to initiate the model, then fit your scaled data. In the data returned from the `.fit` method, assign the `labels_` back to `customers['labels_DBSCAN']`. Now your original data have two labels, one from K-Means and the other from DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import DBSCAN from sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Initialize the DBSCAN model with eps=0.5\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "\n",
    "# Fit the DBSCAN model to the scaled data\n",
    "dbscan.fit(customers_scale)\n",
    "\n",
    "# Assign the DBSCAN cluster labels to the original data\n",
    "data_no_outliers['labels_DBSCAN'] = dbscan.labels_\n",
    "\n",
    "# Display the first few rows to check the DBSCAN labels\n",
    "print(\"\\nSample of the customers with DBSCAN cluster labels:\")\n",
    "print(data_no_outliers.head())\n",
    "\n",
    "# Count the number of data points in each DBSCAN cluster\n",
    "dbscan_cluster_counts = data_no_outliers['labels_DBSCAN'].value_counts()\n",
    "print(\"\\nDBSCAN cluster distribution:\")\n",
    "print(dbscan_cluster_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels_DBSCAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of data points in each DBSCAN cluster\n",
    "dbscan_cluster_counts = data_no_outliers['labels_DBSCAN'].value_counts()\n",
    "\n",
    "# Display the cluster distribution\n",
    "print(\"\\nDBSCAN cluster distribution:\")\n",
    "print(dbscan_cluster_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Compare K-Means with DBSCAN\n",
    "\n",
    "Now we want to visually compare how K-Means and DBSCAN have clustered our data. We will create scatter plots for several columns. For each of the following column pairs, plot a scatter plot using `labels` and another using `labels_DBSCAN`. Put them side by side to compare. Which clustering algorithm makes better sense?\n",
    "\n",
    "Columns to visualize:\n",
    "\n",
    "* `Detergents_Paper` as X and `Milk` as y\n",
    "* `Grocery` as X and `Fresh` as y\n",
    "* `Frozen` as X and `Delicassen` as y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Detergents_Paper` as X and `Milk` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x,y,hue):\n",
    "    sns.scatterplot(x=x, \n",
    "                    y=y,\n",
    "                    hue=hue)\n",
    "    plt.title('Detergents Paper vs Milk ')\n",
    "    return plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the plot function\n",
    "def plot(x, y, hue, title):\n",
    "    sns.scatterplot(x=x, y=y, hue=hue, palette='viridis')\n",
    "    plt.title(title)\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "# Set up the figure for side-by-side plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot Detergents_Paper vs Milk for K-Means and DBSCAN\n",
    "sns.scatterplot(ax=axes[0], x=data_no_outliers['Detergents_Paper'], y=data_no_outliers['Milk'], hue=data_no_outliers['Label'], palette='viridis')\n",
    "axes[0].set_title(\"K-Means: Detergents_Paper vs Milk\")\n",
    "axes[0].legend(title='Cluster')\n",
    "\n",
    "sns.scatterplot(ax=axes[1], x=data_no_outliers['Detergents_Paper'], y=data_no_outliers['Milk'], hue=data_no_outliers['labels_DBSCAN'], palette='viridis')\n",
    "axes[1].set_title(\"DBSCAN: Detergents_Paper vs Milk\")\n",
    "axes[1].legend(title='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Grocery vs Fresh for K-Means and DBSCAN\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "sns.scatterplot(ax=axes[0], x=data_no_outliers['Grocery'], y=data_no_outliers['Fresh'], hue=data_no_outliers['Label'], palette='viridis')\n",
    "axes[0].set_title(\"K-Means: Grocery vs Fresh\")\n",
    "axes[0].legend(title='Cluster')\n",
    "\n",
    "sns.scatterplot(ax=axes[1], x=data_no_outliers['Grocery'], y=data_no_outliers['Fresh'], hue=data_no_outliers['labels_DBSCAN'], palette='viridis')\n",
    "axes[1].set_title(\"DBSCAN: Grocery vs Fresh\")\n",
    "axes[1].legend(title='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Frozen vs Delicassen for K-Means and DBSCAN\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "sns.scatterplot(ax=axes[0], x=data_no_outliers['Frozen'], y=data_no_outliers['Delicassen'], hue=data_no_outliers['Label'], palette='viridis')\n",
    "axes[0].set_title(\"K-Means: Frozen vs Delicassen\")\n",
    "axes[0].legend(title='Cluster')\n",
    "\n",
    "sns.scatterplot(ax=axes[1], x=data_no_outliers['Frozen'], y=data_no_outliers['Delicassen'], hue=data_no_outliers['labels_DBSCAN'], palette='viridis')\n",
    "axes[1].set_title(\"DBSCAN: Frozen vs Delicassen\")\n",
    "axes[1].legend(title='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Grocery` as X and `Fresh` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure for side-by-side plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot Grocery vs Fresh for K-Means\n",
    "sns.scatterplot(ax=axes[0], x=data_no_outliers['Grocery'], y=data_no_outliers['Fresh'], hue=data_no_outliers['Label'], palette='viridis')\n",
    "axes[0].set_title(\"K-Means: Grocery vs Fresh\")\n",
    "axes[0].legend(title='Cluster')\n",
    "\n",
    "# Plot Grocery vs Fresh for DBSCAN\n",
    "sns.scatterplot(ax=axes[1], x=data_no_outliers['Grocery'], y=data_no_outliers['Fresh'], hue=data_no_outliers['labels_DBSCAN'], palette='viridis')\n",
    "axes[1].set_title(\"DBSCAN: Grocery vs Fresh\")\n",
    "axes[1].legend(title='Cluster')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Frozen` as X and `Delicassen` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure for side-by-side plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot Frozen vs Delicassen for K-Means\n",
    "sns.scatterplot(ax=axes[0], x=data_no_outliers['Frozen'], y=data_no_outliers['Delicassen'], hue=data_no_outliers['Label'], palette='viridis')\n",
    "axes[0].set_title(\"K-Means: Frozen vs Delicassen\")\n",
    "axes[0].legend(title='Cluster')\n",
    "\n",
    "# Plot Frozen vs Delicassen for DBSCAN\n",
    "sns.scatterplot(ax=axes[1], x=data_no_outliers['Frozen'], y=data_no_outliers['Delicassen'], hue=data_no_outliers['labels_DBSCAN'], palette='viridis')\n",
    "axes[1].set_title(\"DBSCAN: Frozen vs Delicassen\")\n",
    "axes[1].legend(title='Cluster')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a groupby to see how the mean differs between the groups. Group `customers` by `labels` and `labels_DBSCAN` respectively and compute the means for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by K-Means labels and compute the mean for all columns\n",
    "mean_kmeans = data_no_outliers.groupby('Label').mean()\n",
    "\n",
    "# Group by DBSCAN labels and compute the mean for all columns\n",
    "mean_dbscan = data_no_outliers.groupby('labels_DBSCAN').mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Mean values for K-Means clusters:\")\n",
    "print(mean_kmeans)\n",
    "\n",
    "print(\"\\nMean values for DBSCAN clusters:\")\n",
    "print(mean_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm appears to perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "- Based on the nature of the dataset and the comparison between K-Means and DBSCAN, DBSCAN appears to perform better. This is because DBSCAN is more robust in handling noise and outliers, as it doesn't force every point into a cluster and can identify points that don't fit well as outliers (marked as -1). Additionally, DBSCAN can handle clusters of varying shapes and densities, which K-Means struggles with, especially in cases where clusters are irregular or not well-separated. While K-Means may work well for clean, spherical clusters, DBSCAN provides a more flexible and reliable clustering solution for data with noise or irregular structures. Therefore, DBSCAN is the more appropriate algorithm for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Changing K-Means Number of Clusters\n",
    "\n",
    "As we mentioned earlier, we don't need to worry about the number of clusters with DBSCAN because it automatically decides that based on the parameters we send to it. But with K-Means, we have to supply the `n_clusters` param (if you don't supply `n_clusters`, the algorithm will use `8` by default). You need to know that the optimal number of clusters differs case by case based on the dataset. K-Means can perform badly if the wrong number of clusters is used.\n",
    "\n",
    "In advanced machine learning, data scientists try different numbers of clusters and evaluate the results with statistical measures (read [here](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation)). We are not using statistical measures today but we'll use our eyes instead. In the cells below, experiment with different number of clusters and visualize with scatter plots. What number of clusters seems to work best for K-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 3 - Changing DBSCAN `eps` and `min_samples`\n",
    "\n",
    "Experiment changing the `eps` and `min_samples` params for DBSCAN. See how the results differ with scatter plot visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
